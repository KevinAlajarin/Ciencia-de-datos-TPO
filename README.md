# Sistema ETL de AnÃ¡lisis de E-commerce Brasil

## ðŸ“‹ DescripciÃ³n General

Este proyecto es un sistema completo de procesamiento ETL (Extract, Transform, Load) diseÃ±ado para analizar datos de e-commerce brasileÃ±o utilizando el dataset pÃºblico de Olist. El sistema procesa informaciÃ³n de pedidos, clientes, productos, geolocalizaciÃ³n e indicadores econÃ³micos para determinar ubicaciones Ã³ptimas de almacenes mediante tÃ©cnicas de clustering y anÃ¡lisis avanzado.

## ðŸ—ï¸ Arquitectura del Proyecto

El proyecto estÃ¡ estructurado en dos componentes principales:

### Backend (Python)
- **UbicaciÃ³n**: `backend/procesamiento/`
- **PropÃ³sito**: Procesamiento ETL, anÃ¡lisis de datos y carga en MongoDB
- **TecnologÃ­as**: Python 3.x, Pandas, NumPy, Scikit-learn, PyMongo

### Frontend (React)
- **UbicaciÃ³n**: `frontend/`
- **PropÃ³sito**: VisualizaciÃ³n de datos y resultados
- **TecnologÃ­as**: React 19, Vite, ECharts

## ðŸ“ Estructura de Directorios

```
.
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ procesamiento/
â”‚       â”œâ”€â”€ data/                          # Datasets CSV
â”‚       â”‚   â”œâ”€â”€ olist_orders_dataset.csv
â”‚       â”‚   â”œâ”€â”€ olist_customers_dataset.csv
â”‚       â”‚   â”œâ”€â”€ olist_order_items_dataset.csv
â”‚       â”‚   â”œâ”€â”€ olist_products_dataset.csv
â”‚       â”‚   â”œâ”€â”€ olist_sellers_dataset.csv
â”‚       â”‚   â”œâ”€â”€ olist_geolocation_dataset.csv
â”‚       â”‚   â””â”€â”€ brazil_economy_indicators.csv
â”‚       â”œâ”€â”€ etl/
â”‚       â”‚   â”œâ”€â”€ config.py                  # ConfiguraciÃ³n de rutas y MongoDB
â”‚       â”‚   â”œâ”€â”€ database/
â”‚       â”‚   â”‚   â”œâ”€â”€ mongo_handler.py       # Manejo de conexiÃ³n MongoDB
â”‚       â”‚   â”‚   â””â”€â”€ create_economic_collection.py
â”‚       â”‚   â””â”€â”€ processing/
â”‚       â”‚       â”œâ”€â”€ data_cleaner.py         # Limpieza y carga de datos
â”‚       â”‚       â”œâ”€â”€ data_processor.py       # Orquestador principal ETL
â”‚       â”‚       â”œâ”€â”€ metric_calculator.py    # CÃ¡lculo de mÃ©tricas
â”‚       â”‚       â”œâ”€â”€ warehouse_allocator.py  # Clustering y ubicaciÃ³n de almacenes
â”‚       â”‚       â”œâ”€â”€ economic_analyzer.py   # AnÃ¡lisis econÃ³mico
â”‚       â”‚       â””â”€â”€ delivery_analyzer.py   # AnÃ¡lisis de entregas
â”‚       â”œâ”€â”€ main.py                         # Punto de entrada
â”‚       â”œâ”€â”€ requirements.txt                # Dependencias Python
â”‚       â””â”€â”€ test_connection.py             # Script de prueba MongoDB
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ App.jsx                         # Componente principal
    â”‚   â”œâ”€â”€ HistoricPage.jsx               # PÃ¡gina histÃ³rica
    â”‚   â””â”€â”€ main.jsx                        # Entry point React
    â”œâ”€â”€ package.json                        # Dependencias Node.js
    â””â”€â”€ vite.config.js                      # ConfiguraciÃ³n Vite
```

## ðŸ”„ Flujo de Procesamiento ETL

### Fase 1: ExtracciÃ³n y Limpieza (Extract & Transform)

El proceso comienza en `main.py` y sigue estos pasos:

1. **Carga de Datasets** (`DataCleaner.load_all_datasets()`)
   - Carga 7 archivos CSV desde `backend/procesamiento/data/`
   - Datasets: Ã³rdenes, clientes, items, productos, vendedores, geolocalizaciÃ³n, indicadores econÃ³micos

2. **Filtrado de Datos** (`DataCleaner.filter_delivered_orders()`)
   - Filtra solo Ã³rdenes con estado "delivered"
   - Elimina datos incompletos o invÃ¡lidos

3. **Limpieza de Datos** (`DataCleaner.clean_datasets()`)
   - NormalizaciÃ³n de formatos de fecha
   - Manejo de valores nulos
   - ValidaciÃ³n de tipos de datos

### Fase 2: Procesamiento y AnÃ¡lisis (Transform)

El `DataProcessor` ejecuta mÃºltiples anÃ¡lisis:

#### 2.1 CÃ¡lculo de MÃ©tricas (`MetricCalculator`)

- **MÃ©tricas Generales**:
  - Total de clientes Ãºnicos
  - Total de items vendidos
  - Promedio de items por cliente

- **AnÃ¡lisis de Entregas** (`DeliveryAnalyzer`):
  - CÃ¡lculo de dÃ­as de entrega
  - ClasificaciÃ³n: rÃ¡pida, media, lenta (percentiles 25, 50, 75)
  - EstadÃ­sticas por estado brasileÃ±o
  - Tendencias temporales de velocidad de entrega

- **AnÃ¡lisis EconÃ³mico** (`EconomicAnalyzer`):
  - CorrelaciÃ³n entre volumen de pedidos e indicadores econÃ³micos:
    - Actividad econÃ³mica (`econ_act`)
    - Deuda pÃºblica (`peo_debt`)
    - InflaciÃ³n (`inflation`)
    - Tasa de interÃ©s (`interest_rate`)
  - VolÃºmenes mensuales de pedidos
  - Tendencias de crecimiento/decrecimiento

#### 2.2 Clustering para UbicaciÃ³n de Almacenes (`WarehouseAllocator`)

El sistema utiliza **tres algoritmos de clustering** para determinar ubicaciones Ã³ptimas:

1. **KMeans** (ClÃ¡sico)
   - SelecciÃ³n automÃ¡tica de clusters mediante mÃ©todo del codo
   - Rango de bÃºsqueda: 5 a mÃ¡ximo adaptativo (sqrt(n_puntos) / 2)

2. **MiniBatchKMeans** (Optimizado para grandes datasets)
   - Mismo mÃ©todo de selecciÃ³n que KMeans
   - Batch size: 2048
   - MÃ¡s eficiente en memoria

3. **GMM (Gaussian Mixture Model)**
   - SelecciÃ³n mediante criterio BIC (Bayesian Information Criterion)
   - Rango adaptativo: mÃ­nimo 25, mÃ¡ximo 75 clusters

**Proceso de Clustering**:

1. **PreparaciÃ³n de Coordenadas**:
   - Merge de clientes con geolocalizaciÃ³n por cÃ³digo postal
   - ExtracciÃ³n de coordenadas lat/lng vÃ¡lidas

2. **Clustering Principal**:
   - AplicaciÃ³n del algoritmo seleccionado
   - AsignaciÃ³n de cada cliente a un cluster

3. **CÃ¡lculo de Centros de Almacenes**:
   - Para cada cluster:
     - CÃ¡lculo del centroide geogrÃ¡fico
     - EliminaciÃ³n de outliers (percentil 95)
     - CÃ¡lculo de densidad de clientes
     - IdentificaciÃ³n de productos mÃ¡s vendidos

4. **Subclustering AutomÃ¡tico**:
   - Si un cluster tiene densidad > 8% del total:
     - Se divide automÃ¡ticamente en subclusters (mÃ¡ximo 3)
     - Permite mayor granularidad en zonas de alta densidad

5. **ClasificaciÃ³n de TamaÃ±o**:
   - **Large**: densidad > 4% del total
   - **Medium**: densidad entre 1.5% y 4%
   - **Small**: densidad < 1.5%

6. **EstimaciÃ³n de Mejora de Entrega**:
   - CÃ¡lculo de mejora porcentual estimada (10% - 25%)
   - Basado en la densidad del cluster

#### 2.3 ProyecciÃ³n de Crecimiento

Para cada almacÃ©n, se calcula crecimiento proyectado a 1 y 2 aÃ±os:

```
growth_factor = 0.5 * econ_act - 0.2 * peo_debt - 0.1 * inflation - 0.1 * interest_rate
growth_factor = clamp(growth_factor, -0.5, 1.0)

estimated_customers_1y = current_customers * (1 + growth_factor)
estimated_customers_2y = current_customers * (1 + growth_factor)Â²
```

### Fase 3: Carga en MongoDB (Load)

El `MongoDBHandler` gestiona la persistencia:

1. **ConexiÃ³n**:
   - URI de MongoDB desde variable de entorno `MONGODB_URI`
   - Base de datos: `ecommerce_brazil` (configurable)

2. **Colecciones Creadas**:
   - `orders`: Ã“rdenes procesadas
   - `order_items`: Items de cada orden
   - `customers`: InformaciÃ³n de clientes
   - `sellers`: InformaciÃ³n de vendedores
   - `products`: CatÃ¡logo de productos
   - `geolocation`: Datos geogrÃ¡ficos
   - `economic_data`: Indicadores econÃ³micos
   - `processed_results_kmeans`: Resultados del modelo KMeans
   - `processed_results_minibatch`: Resultados del modelo MiniBatchKMeans
   - `processed_results_gmm`: Resultados del modelo GMM

3. **Estructura de `processed_results_*`**:
```json
{
  "timestamp": "ISO 8601",
  "metrics": {
    "total_customers": int,
    "total_items": int,
    "items_per_customer_avg": float,
    "total_warehouses": int,
    "avg_customers_per_warehouse": int
  },
  "economic_analysis": {
    "national_correlations": {...},
    "monthly_volumes": [...],
    "trend_estimates": {...}
  },
  "delivery_stats": {
    "summary": {...},
    "by_state": [...]
  },
  "warehouses": [
    {
      "warehouse_id": str/int,
      "latitude": float,
      "longitude": float,
      "customer_count": int,
      "density_ratio": float,
      "warehouse_size": "large|medium|small",
      "estimated_delivery_improvement_%": float,
      "top_items": [str],
      "estimated_customer_growth_1y": int,
      "estimated_customer_growth_2y": int,
      "note": str,
      "algorithm": "kmeans|minibatch|gmm"
    }
  ],
  "cluster_logs": [
    {
      "algorithm": str,
      "total_warehouses": int,
      "large": int,
      "medium": int,
      "small": int
    }
  ],
  "notes": {
    "clustering_method": str,
    "n_clusters": int
  }
}
```

## ðŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos

- Python 3.8+
- Node.js 16+ (para frontend)
- MongoDB Atlas (o MongoDB local)
- Cuenta de MongoDB Atlas con cluster creado

### Backend

1. **Instalar dependencias**:
```bash
cd backend/procesamiento
pip install -r requirements.txt
```

2. **Configurar variables de entorno**:
Crear archivo `.env` en `backend/procesamiento/`:
```
MONGODB_URI=
MONGODB_DATABASE=
```

3. **Ejecutar el proceso ETL**:
```bash
python main.py
```

### Frontend

1. **Instalar dependencias**:
```bash
cd frontend
npm install
```

2. **Ejecutar en modo desarrollo**:
```bash
npm run dev
```

3. **Compilar para producciÃ³n**:
```bash
npm run build
```

## ðŸ“Š Datasets Utilizados

### Datasets Olist
- **orders**: InformaciÃ³n de pedidos (estado, fechas, cliente)
- **order_items**: Items individuales de cada pedido
- **customers**: Datos demogrÃ¡ficos de clientes
- **sellers**: InformaciÃ³n de vendedores
- **products**: CatÃ¡logo de productos
- **geolocation**: Coordenadas geogrÃ¡ficas por cÃ³digo postal

### Dataset EconÃ³mico
- **brazil_economy_indicators**: Indicadores macroeconÃ³micos mensuales de Brasil

## ðŸ” Algoritmos de Clustering

### KMeans
- **Ventaja**: RÃ¡pido y eficiente
- **Uso**: Dataset estÃ¡ndar
- **SelecciÃ³n de K**: MÃ©todo del codo con segunda derivada

### MiniBatchKMeans
- **Ventaja**: Optimizado para grandes volÃºmenes de datos
- **Uso**: Cuando el dataset es muy grande
- **Batch Size**: 2048

### GMM (Gaussian Mixture Model)
- **Ventaja**: Modela distribuciones probabilÃ­sticas mÃ¡s complejas
- **Uso**: Cuando se esperan clusters con formas no esfÃ©ricas
- **SelecciÃ³n de componentes**: Criterio BIC

## ðŸ“ˆ MÃ©tricas y AnÃ¡lisis Generados

### MÃ©tricas de Negocio
- Total de clientes Ãºnicos
- Total de items vendidos
- Promedio de items por cliente
- Total de almacenes recomendados
- Promedio de clientes por almacÃ©n

### AnÃ¡lisis de Entregas
- DistribuciÃ³n de velocidad (rÃ¡pida/media/lenta)
- Tiempo promedio de entrega por estado
- Tendencias temporales

### AnÃ¡lisis EconÃ³mico
- Correlaciones entre pedidos e indicadores econÃ³micos
- VolÃºmenes mensuales histÃ³ricos
- Tendencias de crecimiento/decrecimiento

### AnÃ¡lisis GeogrÃ¡fico
- Ubicaciones Ã³ptimas de almacenes (lat/lng)
- Densidad de clientes por regiÃ³n
- Productos mÃ¡s vendidos por regiÃ³n
- ProyecciÃ³n de crecimiento de clientes

## ðŸ› ï¸ MÃ³dulos Principales

### `DataCleaner`
Responsable de:
- Carga de archivos CSV
- Filtrado de datos relevantes
- Limpieza y normalizaciÃ³n

### `DataProcessor`
Orquestador principal que:
- Coordina el flujo ETL completo
- Ejecuta los tres modelos de clustering
- Genera resultados estructurados

### `MetricCalculator`
Calcula:
- MÃ©tricas generales del negocio
- EstadÃ­sticas de entregas
- AnÃ¡lisis econÃ³mico y correlaciones

### `WarehouseAllocator`
Implementa:
- Tres algoritmos de clustering
- SelecciÃ³n automÃ¡tica de nÃºmero de clusters
- CÃ¡lculo de ubicaciones Ã³ptimas
- ClasificaciÃ³n por tamaÃ±o

### `EconomicAnalyzer`
Analiza:
- Correlaciones econÃ³micas
- Tendencias temporales
- VolÃºmenes mensuales

### `DeliveryAnalyzer`
EvalÃºa:
- Performance de entregas
- ClasificaciÃ³n por velocidad
- EstadÃ­sticas por estado

### `MongoDBHandler`
Gestiona:
- ConexiÃ³n a MongoDB
- InserciÃ³n de colecciones
- Limpieza de colecciones existentes

## ðŸ”§ ConfiguraciÃ³n Avanzada

### Ajustar NÃºmero de Clusters

En `main.py`, puedes especificar el nÃºmero de clusters:
```python
processor = DataProcessor()
processor.execute_etl(n_clusters=20)  # Fuerza 20 clusters
```

Si no se especifica, el sistema selecciona automÃ¡ticamente el Ã³ptimo.

### Modificar ParÃ¡metros de Clustering

Editar `warehouse_allocator.py`:
- `max_clusters`: LÃ­mite superior para bÃºsqueda
- `batch_size`: TamaÃ±o de batch para MiniBatchKMeans
- Umbrales de densidad para clasificaciÃ³n de tamaÃ±os

### Personalizar ProyecciÃ³n de Crecimiento

En `data_processor.py`, funciÃ³n `apply_growth()`:
```python
growth_factor = 0.5 * norm_econ_act - 0.2 * norm_peo_debt - 0.1 * norm_inflation - 0.1 * norm_interest_rate
```
Ajustar los coeficientes segÃºn necesidades del negocio.

